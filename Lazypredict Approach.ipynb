{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "039f187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('C:/Users/IDEH/Desktop/Sentimental/Data/Sentiment140.csv',\n",
    "                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da056b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_map = {0: 0, 4: 1}  # 0: negative, 1: positive\n",
    "# dataset.loc[:, 'sentiment'] = dataset['sentiment'].map(sentiment_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ef3b934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>557138</th>\n",
       "      <td>0</td>\n",
       "      <td>2204444171</td>\n",
       "      <td>Wed Jun 17 02:14:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>einmensch</td>\n",
       "      <td>wants to compete! i want hard competition! i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349381</th>\n",
       "      <td>0</td>\n",
       "      <td>2017152437</td>\n",
       "      <td>Wed Jun 03 07:56:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>keithmorrison</td>\n",
       "      <td>It seems we are stuck on the ground in Amarill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182051</th>\n",
       "      <td>0</td>\n",
       "      <td>1967043408</td>\n",
       "      <td>Fri May 29 18:52:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PunkieDory</td>\n",
       "      <td>where the f are my pinking shears? rarararrrar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571236</th>\n",
       "      <td>0</td>\n",
       "      <td>2208721054</td>\n",
       "      <td>Wed Jun 17 09:32:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DYkEY_tYPE</td>\n",
       "      <td>0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339637</th>\n",
       "      <td>4</td>\n",
       "      <td>2018731586</td>\n",
       "      <td>Wed Jun 03 10:25:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BlueSmartiies</td>\n",
       "      <td>@ reply me pls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92222</th>\n",
       "      <td>0</td>\n",
       "      <td>1759931251</td>\n",
       "      <td>Sun May 10 20:16:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SteveCoulson</td>\n",
       "      <td>@amandagravel aww you get &amp;quot;saddest tweet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488542</th>\n",
       "      <td>4</td>\n",
       "      <td>2068514157</td>\n",
       "      <td>Sun Jun 07 14:12:42 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Genelly1238</td>\n",
       "      <td>@mitchelmusso ,,hey youre ausome. please give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657354</th>\n",
       "      <td>0</td>\n",
       "      <td>2240992680</td>\n",
       "      <td>Fri Jun 19 10:24:42 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>firebuilt</td>\n",
       "      <td>Laptop got a virus, kept crashing, had to rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368785</th>\n",
       "      <td>4</td>\n",
       "      <td>2050677502</td>\n",
       "      <td>Fri Jun 05 19:05:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CICELYCORINNE</td>\n",
       "      <td>@oOoshecutee hey hey now!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246690</th>\n",
       "      <td>4</td>\n",
       "      <td>1995383092</td>\n",
       "      <td>Mon Jun 01 13:24:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jeromejtk</td>\n",
       "      <td>@MagnusApollo maybe not as frustrating as your...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment         ids                          date      flag  \\\n",
       "557138           0  2204444171  Wed Jun 17 02:14:00 PDT 2009  NO_QUERY   \n",
       "349381           0  2017152437  Wed Jun 03 07:56:34 PDT 2009  NO_QUERY   \n",
       "182051           0  1967043408  Fri May 29 18:52:13 PDT 2009  NO_QUERY   \n",
       "571236           0  2208721054  Wed Jun 17 09:32:48 PDT 2009  NO_QUERY   \n",
       "1339637          4  2018731586  Wed Jun 03 10:25:27 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "92222            0  1759931251  Sun May 10 20:16:36 PDT 2009  NO_QUERY   \n",
       "1488542          4  2068514157  Sun Jun 07 14:12:42 PDT 2009  NO_QUERY   \n",
       "657354           0  2240992680  Fri Jun 19 10:24:42 PDT 2009  NO_QUERY   \n",
       "1368785          4  2050677502  Fri Jun 05 19:05:57 PDT 2009  NO_QUERY   \n",
       "1246690          4  1995383092  Mon Jun 01 13:24:10 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user                                               text  \n",
       "557138       einmensch  wants to compete! i want hard competition! i w...  \n",
       "349381   keithmorrison  It seems we are stuck on the ground in Amarill...  \n",
       "182051      PunkieDory  where the f are my pinking shears? rarararrrar...  \n",
       "571236      DYkEY_tYPE  0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...  \n",
       "1339637  BlueSmartiies                                    @ reply me pls   \n",
       "...                ...                                                ...  \n",
       "92222     SteveCoulson  @amandagravel aww you get &quot;saddest tweet ...  \n",
       "1488542    Genelly1238  @mitchelmusso ,,hey youre ausome. please give ...  \n",
       "657354       firebuilt  Laptop got a virus, kept crashing, had to rest...  \n",
       "1368785  CICELYCORINNE                        @oOoshecutee hey hey now!!   \n",
       "1246690      jeromejtk  @MagnusApollo maybe not as frustrating as your...  \n",
       "\n",
       "[1600 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample the dataset to reduce size\n",
    "dataset = dataset.sample(frac=0.001, random_state=0)  # Use 10% of the data\n",
    "dataset.to_csv(\"160ksentimentdata2.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1377e1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1600 entries, 557138 to 1246690\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  1600 non-null   int64 \n",
      " 1   ids        1600 non-null   int64 \n",
      " 2   date       1600 non-null   object\n",
      " 3   flag       1600 non-null   object\n",
      " 4   user       1600 non-null   object\n",
      " 5   text       1600 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 87.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962ffdd",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4429f14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|█████████████████████████████████████████████████| 160000/160000 [00:11<00:00, 13781.56tweet/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed in 11.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Define your custom preprocessing function with tqdm and time\n",
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern = '@[^\\s]+'\n",
    "    alphaPattern = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for tweet in tqdm(textdata, desc=\"Processing tweets\", unit=\"tweet\"):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(urlPattern, ' URL', tweet)\n",
    "        tweet = re.sub(userPattern, ' USER', tweet)\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            word = wordLemm.lemmatize(word)\n",
    "            tweetwords += (word + ' ')\n",
    "        processedText.append(tweetwords)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Preprocessing completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return processedText\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "processed_text = preprocess(dataset['text'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a889c08",
   "metadata": {},
   "source": [
    " Feature Extraction : TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3327d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing text data: 100%|██████████████████████████████████████████| 160000/160000 [00:02<00:00, 57650.72document/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization completed in 5.08 seconds\n",
      "Data splitting completed in 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
    "\n",
    "# Measure the time taken for vectorization\n",
    "start_time = time.time()\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(tqdm(processed_text, desc=\"Vectorizing text data\", unit=\"document\"))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Vectorization completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "start_time = time.time()\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, dataset['sentiment'].values, test_size=0.2, random_state=0)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Data splitting completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d70c1540",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.54 GiB for an array with shape (128000, 10000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_dense \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_tfidf\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[0;32m      2\u001b[0m X_test_dense \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test_tfidf\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.54 GiB for an array with shape (128000, 10000) and data type float64"
     ]
    }
   ],
   "source": [
    "X_train_dense = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "X_test_dense = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7ba89",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, predictions = clf.fit(X_train_tfidf, X_test_tfidf, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41cb019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|█████████████████████████████████████████████████| 160000/160000 [00:10<00:00, 14872.56tweet/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed in 10.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing text data: 100%|██████████████████████████████████████████| 160000/160000 [00:02<00:00, 59487.37document/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization completed in 4.94 seconds\n",
      "Data splitting completed in 0.02 seconds\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "select_dtypes not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Initialize and fit the LazyClassifier with sparse matrices\u001b[39;00m\n\u001b[0;32m     79\u001b[0m clf \u001b[38;5;241m=\u001b[39m LazyClassifier(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, custom_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 80\u001b[0m models, predictions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mfit(X_train_tfidf, X_test_tfidf, y_train, y_test)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Display the models\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(models)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\lazypredict\\Supervised.py:260\u001b[0m, in \u001b[0;36mLazyClassifier.fit\u001b[1;34m(self, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m    257\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train)\n\u001b[0;32m    258\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_test)\n\u001b[1;32m--> 260\u001b[0m numeric_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    261\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    263\u001b[0m categorical_low, categorical_high \u001b[38;5;241m=\u001b[39m get_card_split(\n\u001b[0;32m    264\u001b[0m     X_train, categorical_features\n\u001b[0;32m    265\u001b[0m )\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\scipy\\sparse\\_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: select_dtypes not found"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# Load the dataset\n",
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('C:/Users/IDEH/Desktop/Sentimental/Data/Sentiment140.csv',\n",
    "                      encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "\n",
    "# Map sentiment values\n",
    "sentiment_map = {0: 0, 4: 1}  # 0: negative, 1: positive\n",
    "dataset.loc[:, 'sentiment'] = dataset['sentiment'].map(sentiment_map)\n",
    "\n",
    "# Sample the dataset to reduce size\n",
    "dataset = dataset.sample(frac=0.1, random_state=0)  # Use 10% of the data\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern = '@[^\\s]+'\n",
    "    alphaPattern = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for tweet in tqdm(textdata, desc=\"Processing tweets\", unit=\"tweet\"):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(urlPattern, ' URL', tweet)\n",
    "        tweet = re.sub(userPattern, ' USER', tweet)\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            word = wordLemm.lemmatize(word)\n",
    "            tweetwords += (word + ' ')\n",
    "        processedText.append(tweetwords)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Preprocessing completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return processedText\n",
    "\n",
    "# Apply the preprocessing function to the text data\n",
    "processed_text = preprocess(dataset['text'].values)\n",
    "\n",
    "# Initialize the TfidfVectorizer with reduced features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
    "\n",
    "# Measure the time taken for vectorization\n",
    "start_time = time.time()\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(tqdm(processed_text, desc=\"Vectorizing text data\", unit=\"document\"))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Vectorization completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "start_time = time.time()\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, dataset['sentiment'].values, test_size=0.2, random_state=0)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Data splitting completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Initialize and fit the LazyClassifier with sparse matrices\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "\n",
    "# Display the models\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f58e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
