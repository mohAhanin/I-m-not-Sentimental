{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c630f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('C:/Users/IDEH/Desktop/Sentiment 140/Data/Sentiment140.csv',\n",
    "                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8054da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['sentiment','text']]\n",
    "# Finding the value counts of the sentiment column\n",
    "dataset['sentiment'] = dataset['sentiment'].replace(4,1)\n",
    "# Finding the value counts of the sentiment column\n",
    "sentiment_counts = dataset['sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef68f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution for dataset.\n",
    "ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n",
    "                                               legend=False)\n",
    "ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
    "\n",
    "# Storing data in lists.\n",
    "text, sentiment = list(dataset['text']), list(dataset['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing data in lists.\n",
    "text, sentiment = list(dataset['text']), list(dataset['sentiment'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7426a36",
   "metadata": {},
   "source": [
    "The Preprocessing steps taken are:\n",
    "\n",
    "Lower Casing: Each text is converted to lowercase.\n",
    "Replacing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"URL\".\n",
    "Replacing Emojis: Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. (eg: \":)\" to \"EMOJIsmile\")\n",
    "Replacing Usernames: Replace @Usernames with word \"USER\". (eg: \"@Kaggle\" to \"USER\")\n",
    "Removing Non-Alphabets: Replacing characters except Digits and Alphabets with a space.\n",
    "Removing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. (eg: \"Heyyyy\" to \"Heyy\")\n",
    "Removing Short Words: Words with length less than 2 are removed.\n",
    "Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n",
    "Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: “Great” to “Good”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b24172",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining set containing all stopwords in english.\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    \n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Defining regex patterns.\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL',tweet)\n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)        \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            # Checking if the word is a stopword.\n",
    "            #if word not in stopwordlist:\n",
    "            if len(word)>1:\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7036f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "processedtext = preprocess(text)\n",
    "print(f'Text Preprocessing complete.')\n",
    "print(f'Time Taken: {round(time.time()-t)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a2ab0",
   "metadata": {},
   "source": [
    "Word-Cloud for Negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = processedtext[:800000]\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(data_neg))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce2921",
   "metadata": {},
   "source": [
    "Word-Cloud for Positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = processedtext[800000:]\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "              collocations=False).generate(\" \".join(data_pos))\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072be8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n",
    "                                                    test_size = 0.05, random_state = 0)\n",
    "print(f'Data Split done.')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64126a63",
   "metadata": {},
   "source": [
    "TF-IDF Vectoriser\n",
    "TF-IDF indicates what the importance of the word is in order to understand the document or dataset. Let us understand with an example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears many times; it’s a high frequency word compared to other words in the dataset. The dataset contains other words like home, house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the word. This is the intuition behind TF-IDF.\n",
    "\n",
    "TF-IDF Vectoriser converts a collection of raw documents to a matrix of TF-IDF features. The Vectoriser is usually trained on only the X_train dataset.\n",
    "\n",
    "ngram_range is the range of number of words in a sequence. [e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]\n",
    "\n",
    "max_features specifies the number of features to consider. [Ordered by feature frequency across the corpus]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize and fit the vectorizer\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print(f'Vectoriser fitted.')\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectoriser.get_feature_names_out()\n",
    "print('No. of feature_words: ', len(feature_names))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68ab2cfc",
   "metadata": {},
   "source": [
    "Tranforming the dataset\n",
    "Transforming the X_train and X_test dataset into matrix of TF-IDF Features by using the TF-IDF Vectoriser. This datasets will be used to train the model and test against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f99c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)\n",
    "print(f'Data Transformed.')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9a952b1",
   "metadata": {},
   "source": [
    "We're creating 3 different types of model for our sentiment analysis problem:\n",
    "\n",
    "Bernoulli Naive Bayes (BernoulliNB)\n",
    "Linear Support Vector Classification (LinearSVC)\n",
    "Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed007834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNBmodel = BernoulliNB(alpha = 2)\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCmodel = LinearSVC()\n",
    "SVCmodel.fit(X_train, y_train)\n",
    "model_Evaluate(SVCmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "model_Evaluate(LRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429466a8",
   "metadata": {},
   "source": [
    "# Another Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb205c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc9a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
