{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b116d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d100435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59775e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('C:/Users/IDEH/Desktop/Sentimental/Data/Sentiment140.csv',\n",
    "                      encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de811df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map sentiment labels to integers (0: negative, 4: positive)\n",
    "sentiment_map = {0: 0, 4: 1}  # 0: negative, 1: positive\n",
    "dataset.loc[:, 'sentiment'] = dataset['sentiment'].map(sentiment_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1534222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|████████████████████████████████████████████████| 1600000/1600000 [05:08<00:00, 5194.35tweet/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern = '@[^\\s]+'\n",
    "    alphaPattern = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "    for tweet in tqdm(textdata, desc=\"Processing tweets\", unit=\"tweet\"):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(urlPattern, ' URL', tweet)\n",
    "        tweet = re.sub(userPattern, ' USER', tweet)\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            word = wordLemm.lemmatize(word)\n",
    "            tweetwords += (word + ' ')\n",
    "        processedText.append(tweetwords)\n",
    "\n",
    "    return processedText\n",
    "\n",
    "processed_text = preprocess(dataset['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f5e53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=20000, stop_words='english', ngram_range=(1, 2), min_df=5, max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc785e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing text data: 100%|████████████████████████████████████████| 1600000/1600000 [01:20<00:00, 19993.52document/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization completed in 114.52 seconds\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text data\n",
    "start_time = time.time()\n",
    "X_counts = count_vectorizer.fit_transform(tqdm(processed_text, desc=\"Vectorizing text data\", unit=\"document\"))\n",
    "end_time = time.time()\n",
    "print(f\"Vectorization completed in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c2e3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_counts, dataset['sentiment'].values, test_size=0.2, random_state=0)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa197c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to tensorflow sparse tensor\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64640ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = convert_sparse_matrix_to_sparse_tensor(X_train)\n",
    "X_val_sparse = convert_sparse_matrix_to_sparse_tensor(X_val)\n",
    "X_test_sparse = convert_sparse_matrix_to_sparse_tensor(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0059d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57fb06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the model\n",
    "model = create_model(X_counts.shape[1])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d07892c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f5c1de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_sparse, y_train,\n\u001b[0;32m      3\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m      5\u001b[0m                     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      6\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],\n\u001b[0;32m      7\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1766\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1764\u001b[0m unsplitable \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_arrays \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _can_split(t)]\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsplitable:\n\u001b[1;32m-> 1766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1767\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_split` is only supported for Tensors or NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1768\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrays, found following types in the input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(unsplitable)\n\u001b[0;32m   1769\u001b[0m     )\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_arrays):\n\u001b[0;32m   1772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays, arrays\n",
      "\u001b[1;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_sparse, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=256,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12867e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets:  58%|████████████████████████████▎                    | 926065/1600000 [03:13<02:07, 5266.60tweet/s]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "\n",
    "# Load and preprocess the data\n",
    "DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset = pd.read_csv('C:/Users/IDEH/Desktop/Sentimental/Data/Sentiment140.csv',\n",
    "                      encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "\n",
    "# Map sentiment labels to integers (0: negative, 4: positive)\n",
    "sentiment_map = {0: 0, 4: 1}  # 0: negative, 1: positive\n",
    "dataset.loc[:, 'sentiment'] = dataset['sentiment'].map(sentiment_map)\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern = '@[^\\s]+'\n",
    "    alphaPattern = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "    for tweet in tqdm(textdata, desc=\"Processing tweets\", unit=\"tweet\"):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(urlPattern, ' URL', tweet)\n",
    "        tweet = re.sub(userPattern, ' USER', tweet)\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            word = wordLemm.lemmatize(word)\n",
    "            tweetwords += (word + ' ')\n",
    "        processedText.append(tweetwords)\n",
    "\n",
    "    return processedText\n",
    "\n",
    "processed_text = preprocess(dataset['text'].values)\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=20000, stop_words='english', ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
    "\n",
    "# Vectorize the text data\n",
    "start_time = time.time()\n",
    "X_counts = count_vectorizer.fit_transform(tqdm(processed_text, desc=\"Vectorizing text data\", unit=\"document\"))\n",
    "end_time = time.time()\n",
    "print(f\"Vectorization completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_counts, dataset['sentiment'].values, test_size=0.2, random_state=0)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7147fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|████████████████████████████████████████████████| 1600000/1600000 [05:06<00:00, 5214.62tweet/s]\n",
      "Vectorizing text data: 100%|████████████████████████████████████████| 1600000/1600000 [01:18<00:00, 20428.73document/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization completed in 112.79 seconds\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = [0,522] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_sparse, y_train,\n\u001b[0;32m    102\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    103\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m    104\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_val_sparse, y_val),\n\u001b[0;32m    105\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping],\n\u001b[0;32m    106\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    109\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_sparse)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1] = [0,522] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse] name: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert sparse matrix to tensorflow sparse tensor\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "X_train_sparse = convert_sparse_matrix_to_sparse_tensor(X_train)\n",
    "X_val_sparse = convert_sparse_matrix_to_sparse_tensor(X_val)\n",
    "X_test_sparse = convert_sparse_matrix_to_sparse_tensor(X_test)\n",
    "\n",
    "# Define the model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_model(X_counts.shape[1])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_sparse, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_val_sparse, y_val),\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_sparse)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b51ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
