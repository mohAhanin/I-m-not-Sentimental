{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f391e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f12eebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and preprocess the data\n",
    "# DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "# DATASET_ENCODING = \"ISO-8859-1\"\n",
    "# dataset = pd.read_csv('C:/Users/IDEH/Desktop/Sentimental/Data/Sentiment140.csv',\n",
    "#                       encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aa566df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_map = {0: 0, 4: 1}  # 0: negative, 1: positive\n",
    "# dataset.loc[:, 'sentiment'] = dataset['sentiment'].map(sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ccdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(textdata):\n",
    "#     processedText = []\n",
    "#     wordLemm = WordNetLemmatizer()\n",
    "#     urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "#     userPattern = '@[^\\s]+'\n",
    "#     alphaPattern = \"[^a-zA-Z0-9]\"\n",
    "#     sequencePattern = r\"(.)\\1\\1+\"\n",
    "#     seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "#     for tweet in tqdm(textdata, desc=\"Processing tweets\", unit=\"tweet\"):\n",
    "#         tweet = tweet.lower()\n",
    "#         tweet = re.sub(urlPattern, ' URL', tweet)\n",
    "#         tweet = re.sub(userPattern, ' USER', tweet)\n",
    "#         tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "#         tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "#         tweetwords = ''\n",
    "#         for word in tweet.split():\n",
    "#             word = wordLemm.lemmatize(word)\n",
    "#             tweetwords += (word + ' ')\n",
    "#         processedText.append(tweetwords)\n",
    "\n",
    "#     return processedText\n",
    "\n",
    "# processed_text = preprocess(dataset['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae3b5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading text data: 100%|████████████████████████████████████████████████| 1600000/1600000 [00:00<00:00, 2019183.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data with a progress bar\n",
    "preprocessed_df = pd.read_csv('C:/Users/IDEH/Desktop/Sentimental/Data/preprocessed_data.csv')\n",
    "\n",
    "# Use tqdm to show progress for converting text column to list\n",
    "processed_text = [text for text in tqdm(preprocessed_df['text'], desc=\"Loading text data\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dff878b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sentiment data: 100%|███████████████████████████████████████████| 1600000/1600000 [00:00<00:00, 2781176.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use tqdm to show progress for converting sentiment column to numpy array\n",
    "Y = [sentiment for sentiment in tqdm(preprocessed_df['sentiment'], desc=\"Loading sentiment data\")]\n",
    "Y = np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baed59ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer: 100%|██████████████████████████████████████████████████| 1600000/1600000 [00:37<00:00, 42679.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and Padding\n",
    "max_features = 10000\n",
    "max_len = 20  # Define max_len\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "\n",
    "# Use tqdm to show progress for fitting the tokenizer\n",
    "tokenizer.fit_on_texts(tqdm(processed_text, desc=\"Fitting tokenizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32e21ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting texts to sequences: 100%|██████████████████████████████████████| 1600000/1600000 [00:30<00:00, 52811.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use tqdm to show progress for converting texts to sequences\n",
    "X = [tokenizer.texts_to_sequences([text])[0] for text in tqdm(processed_text, desc=\"Converting texts to sequences\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b38dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding sequences: 100%|████████████████████████████████████████████████| 1600000/1600000 [00:00<00:00, 3155161.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use tqdm to show progress for padding sequences\n",
    "X = pad_sequences(tqdm(X, desc=\"Padding sequences\"), maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82ef7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da7dd0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280000, 20), (320000, 20), (1280000,), (320000,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24406982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating clients:  80%|████████████████████████████████████████████████████             | 8/10 [00:00<00:00, 38.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples for label 1 in client 1, selected 0 samples\n",
      "Not enough samples for label 1 in client 2, selected 0 samples\n",
      "Not enough samples for label 1 in client 3, selected 0 samples\n",
      "Not enough samples for label 1 in client 4, selected 0 samples\n",
      "Not enough samples for label 1 in client 5, selected 0 samples\n",
      "Not enough samples for label 1 in client 6, selected 0 samples\n",
      "Not enough samples for label 1 in client 7, selected 0 samples\n",
      "Not enough samples for label 1 in client 8, selected 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Creating clients: 100%|████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 38.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples for label 1 in client 9, selected 0 samples\n",
      "Not enough samples for label 1 in client 10, selected 0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create 10 clients\n",
    "num_clients = 10\n",
    "clients_data = []\n",
    "samples_per_label = 10  # Number of samples per label per client\n",
    "\n",
    "for i in tqdm(range(num_clients), desc=\"Creating clients\"):\n",
    "    client_X, client_y = [], []\n",
    "    for label in [0, 1]:\n",
    "        idx = np.where(y_train == label)[0]\n",
    "        if len(idx) >= samples_per_label:\n",
    "            selected_idx = np.random.choice(idx, size=samples_per_label, replace=False)\n",
    "        else:\n",
    "            selected_idx = np.random.choice(idx, size=len(idx), replace=False)  # Select all available samples\n",
    "            print(f\"Not enough samples for label {label} in client {i + 1}, selected {len(selected_idx)} samples\")\n",
    "        client_X.extend(X_train[selected_idx])\n",
    "        client_y.extend(y_train[selected_idx])\n",
    "    clients_data.append((np.array(client_X), np.array(client_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760377c5",
   "metadata": {},
   "source": [
    "Initialize Variables:<br/>\n",
    "num_clients = 10: Specifies the number of clients.\n",
    "clients_data = []: Initializes an empty list to store the data for each client.\n",
    "Loop Through Each Client:<br/>\n",
    "for i in range(num_clients): Loops through each client (from 0 to 9).<br/>\n",
    "Initialize Client Data:<br/>\n",
    "client_X, client_y = [], []:\n",
    "<br/> Initializes empty lists to store the features (client_X) and labels (client_y) for the current client.\n",
    "Loop Through Each Label:<br/>\n",
    "for label in [0, 1]:<br/> Loops through each label (0 and 1).<br/>\n",
    "Select Data for Each Label:<br/>\n",
    "idx = np.where(y_train == label)[0]:<br/> Finds the indices of all samples in y_train that have the current label.<br/>\n",
    "selected_idx = np.random.choice(idx, size=10, replace=False): Randomly selects 10 indices from the found indices without replacement.\n",
    "Add Selected Data to Client Data:<br/>\n",
    "client_X.extend(X_train[selected_idx]): Adds the selected features from X_train to client_X.<br/>\n",
    "client_y.extend(y_train[selected_idx]): Adds the selected labels from y_train to client_y.<br/>\n",
    "Store Client Data:<br/>\n",
    "clients_data.append((np.array(client_X), np.array(client_y))): Converts client_X and client_y to numpy arrays and appends them as a tuple to clients_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fd734c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0,    0,    0,    0,    0,   23,  105,  142,   72,\n",
       "         1684,  326,   16,    1,   65,   68,   98, 1530, 2612],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,   54,\n",
       "            5,  616,    7,  176, 3293,   24,  934,   91,   56],\n",
       "        [ 331,    8,   23,  177,   25,   24, 2913,   55, 4824,    1,  148,\n",
       "           72,  104, 2913,   62,   93,    3,    4, 2062,  173],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            1,   20,    5,  622, 7497,   16,    7,  376, 2599],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,  317,  236,   22,    5,\n",
       "          159,  289,   73,   62,  114,   41,  247,   16,   45],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  199,  331,    8, 4118,   82,    5,  509],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    2,   86,   69,    3,  236,  241,  150,   45],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0, 1687,    6,   13,  408],\n",
       "        [   1,   85,   25,  195,    7,  186, 1547,  289,  213,  878,  550,\n",
       "           28,  101,   85,   20,    7,  359,  201,   30,  157],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    1,   68,    3,   39, 2393,   65,  122]]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d077bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "786e404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def federated_learning(X_train, y_train, X_test, y_test, num_clients=10, num_rounds=5, local_epochs=3):\n",
    "    # Create client data\n",
    "    clients_data = []\n",
    "    samples_per_client = len(X_train) // num_clients\n",
    "    for i in tqdm(range(num_clients), desc=\"Creating clients\"):\n",
    "        start = i * samples_per_client\n",
    "        end = (i + 1) * samples_per_client\n",
    "        clients_data.append((X_train[start:end], y_train[start:end]))\n",
    "\n",
    "    # Initialize global model\n",
    "    global_model = create_model()\n",
    "\n",
    "    for round in tqdm(range(num_rounds), desc=\"Federated learning rounds\"):\n",
    "        print(f\"Round {round + 1}/{num_rounds}\")\n",
    "        \n",
    "        client_models = []\n",
    "        \n",
    "        for i, (client_X, client_y) in enumerate(tqdm(clients_data, desc=\"Training clients\", leave=False)):\n",
    "            print(f\"Training on client {i + 1}/{num_clients}\")\n",
    "            client_model = tf.keras.models.clone_model(global_model)\n",
    "            client_model.set_weights(global_model.get_weights())\n",
    "            client_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Compile the model\n",
    "            client_model.fit(client_X, client_y, epochs=local_epochs, batch_size=32, verbose=0)\n",
    "            client_models.append(client_model)\n",
    "        \n",
    "        # Average the models\n",
    "        averaged_weights = [np.zeros_like(w) for w in global_model.get_weights()]\n",
    "        for client_model in tqdm(client_models, desc=\"Averaging models\", leave=False):\n",
    "            client_weights = client_model.get_weights()\n",
    "            for i in range(len(client_weights)):\n",
    "                averaged_weights[i] += client_weights[i]\n",
    "        averaged_weights = [w / num_clients for w in averaged_weights]\n",
    "        \n",
    "        global_model.set_weights(averaged_weights)\n",
    "        \n",
    "        # Evaluate global model\n",
    "        test_loss, test_accuracy = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return global_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72861941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating clients: 100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:00<?, ?it/s]\n",
      "Federated learning rounds:   0%|                                                                 | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training clients:   0%|                                                                         | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on client 1/10\n"
     ]
    }
   ],
   "source": [
    "# Run Federated Learning\n",
    "start_time = time.time()\n",
    "final_model = federated_learning(X_train, y_train, X_test, y_test)\n",
    "end_time = time.time()\n",
    "print(f\"Total training time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614aa62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
