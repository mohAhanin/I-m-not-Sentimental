{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_federated as tff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds', 'my whole body feels itchy and like its on fire ', \"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \"]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the dataset columns and encoding\n",
    "DATASET_COLUMNS = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "\n",
    "# Load the data\n",
    "dataset = []\n",
    "with open('C:/Users/IDEH/Desktop/Sentimental/Data/Sentiment140.csv', encoding=DATASET_ENCODING) as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        dataset.append(row)\n",
    "\n",
    "# Convert the data to a dictionary format\n",
    "data_dict = {col: [] for col in DATASET_COLUMNS}\n",
    "for row in dataset:\n",
    "    for col, value in zip(DATASET_COLUMNS, row):\n",
    "        data_dict[col].append(value)\n",
    "\n",
    "# Example of accessing the data\n",
    "print(data_dict['text'][:5])  # Print the first 5 text entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USER URL aww that s a bummer you shoulda got david carr of third day to do it d ', 'is upset that he can t update his facebook by texting it and might cry a a result school today also blah ', 'USER i dived many time for the ball managed to save 50 the rest go out of bound ', 'my whole body feel itchy and like it on fire ', 'USER no it s not behaving at all i m mad why am i here because i can t see you all over there ']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Load the preprocessed data\n",
    "preprocessed_data = []\n",
    "with open('C:/Users/IDEH/Desktop/Sentimental/Data/preprocessed_data.csv', encoding='ISO-8859-1') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        preprocessed_data.append(row)\n",
    "\n",
    "# Convert the data to a dictionary format\n",
    "preprocessed_dict = {'text': [], 'sentiment': []}\n",
    "for row in preprocessed_data[1:]:  # Skip the header row\n",
    "    preprocessed_dict['text'].append(row[0])\n",
    "    preprocessed_dict['sentiment'].append(row[1])\n",
    "\n",
    "# Example of accessing the preprocessed data\n",
    "print(preprocessed_dict['text'][:5])  # Print the first 5 text entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3   43  185   18   14    6 1160   10 3373   55  839 7876   15 1870\n",
      "    29    4   50    7  153    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  11  776   18   87   33   16  403  201  550  133 2003    7    9  303\n",
      "   395    6    6 1141  151   46  281 1128    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   3    2    1  331   51   13    5  962 1670    4  874 1072    5  486\n",
      "    40   38   15 3036    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   8  448  789   97 2853    9   39    7   17 1117    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   3   42    7   14   28 9402   27   37    2   22  609  119   67    2\n",
      "    92  224    2   33   16   72   10   37  140   71    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "\n",
    "# Fit the tokenizer on the preprocessed text\n",
    "tokenizer.fit_on_texts(preprocessed_dict['text'])\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_dict['text'])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=100)\n",
    "\n",
    "# Example of accessing the tokenized data\n",
    "print(padded_sequences[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   3   43  185   18   14    6 1160   10 3373   55  839 7876   15 1870\n",
      "    29    4   50    7  153    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  11  776   18   87   33   16  403  201  550  133 2003    7    9  303\n",
      "   395    6    6 1141  151   46  281 1128    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   3    2    1  331   51   13    5  962 1670    4  874 1072    5  486\n",
      "    40   38   15 3036    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   8  448  789   97 2853    9   39    7   17 1117    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   3   42    7   14   28 9402   27   37    2   22  609  119   67    2\n",
      "    92  224    2   33   16   72   10   37  140   71    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]] [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert sentiment labels to integers\n",
    "sentiment_labels = list(map(int, preprocessed_dict['sentiment']))\n",
    "\n",
    "# Split the data into 10 balanced subsets\n",
    "num_clients = 10\n",
    "data_per_client = len(padded_sequences) // num_clients\n",
    "\n",
    "clients_data = []\n",
    "for i in range(num_clients):\n",
    "    start_idx = i * data_per_client\n",
    "    end_idx = (i + 1) * data_per_client\n",
    "    clients_data.append((padded_sequences[start_idx:end_idx], sentiment_labels[start_idx:end_idx]))\n",
    "\n",
    "# Example of accessing the data for the first client\n",
    "client_1_data, client_1_labels = clients_data[0]\n",
    "print(client_1_data[:5], client_1_labels[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Convert client data to tf.data.Dataset\n",
    "def create_tf_dataset_for_client(data, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "    return dataset.batch(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tf.data.Dataset for each client\n",
    "clients_tf_data = [create_tf_dataset_for_client(data, labels) for data, labels in clients_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=10000, output_dim=64, input_length=100),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a TFF model wrapper\n",
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=(tf.TensorSpec(shape=[None, 100], dtype=tf.int32),\n",
    "                    tf.TensorSpec(shape=[None], dtype=tf.int32)),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated learning process\n",
    "def federated_averaging_process():\n",
    "    iterative_process = tff.learning.build_federated_averaging_process(\n",
    "        model_fn=model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
    "    )\n",
    "    return iterative_process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the process\n",
    "iterative_process = federated_averaging_process()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate federated learning\n",
    "state = iterative_process.initialize()\n",
    "num_rounds = 2  # Define the number of rounds\n",
    "for round_num in range(1, num_rounds + 1):\n",
    "    state, metrics = iterative_process.next(state, clients_tf_data)\n",
    "    print(f'Round {round_num}, Metrics: {metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_fed_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
